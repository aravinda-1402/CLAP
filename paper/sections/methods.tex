CLAP runs as a single pipeline from configuration: (1) \textbf{Data generation}---a seeded generator produces $N$ base cases across 10 clinical domains (anticoagulation, diabetes, hypertension, asthma/COPD, CKD dosing, pregnancy meds, infection/antibiotics, pain/opioids, CHF, thyroid). Each base case yields up to four counterfactual variants: renal impairment change, pregnancy toggle (when applicable), allergy to a key medication, and an introduced drug interaction. Base and variant records conform to JSON schemas in \texttt{data/schema/}. (2) \textbf{Suites}---NRT-100 (100 safety-critical cases with required \texttt{risk\_flags}), Ambiguity (cases designed for high uncertainty), and Policy-conflict (refusal/caveat expected). (3) \textbf{Adapters}---OpenAI-compatible API or a deterministic MockModel; outputs are cached by hash(prompt, model, version). (4) \textbf{Prompt contract}---each case is sent with a system instruction that this is synthetic evaluation only; the model must respond with strict JSON (diagnosis, medications, monitoring, contraindications\_flagged, risk\_flags, icd10\_codes, uncertainty). (5) \textbf{Parsing and repair}---responses are validated against a JSON schema; limited repair attempts (e.g., trailing commas) are applied and recorded (valid\_json, repaired, repair\_attempts). (6) \textbf{Metrics and gating}---CFC, SNG, FC, PC are computed; gates (e.g., NRT pass rate 100\%, JSON validity $\ge$ 95\%, canary leakage $\le$ 1\%, CFC $\ge$ threshold) yield a single PASS/FAIL and a list of gate failures. (7) \textbf{Audit packet}---JSON and PDF report with metadata (git hash, config hash, env), gating outcome, suite summaries, worst failures (canaries redacted), and figure/table references.
